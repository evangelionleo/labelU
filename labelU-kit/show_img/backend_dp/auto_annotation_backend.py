#!/usr/bin/env python3
"""
è‡ªåŠ¨æ ‡æ³¨åç«¯æœåŠ¡
ç»“åˆGrounding DINOå’ŒSAM2å®ç°æ–‡æœ¬é©±åŠ¨çš„è‡ªåŠ¨æ ‡æ³¨
æ¥æ”¶å›¾ç‰‡å’Œæ–‡æœ¬ï¼Œè¿”å›æ‰€æœ‰åŒ¹é…å¯¹è±¡çš„bboxå’Œæ©ç 
"""

import os
import uuid
import base64
import io
import numpy as np
from PIL import Image
import cv2
from flask import Flask, request, jsonify, send_from_directory
from flask_cors import CORS
import tempfile
import json
from datetime import datetime

# å…ˆå°è¯•å¯¼å…¥torch
try:
    import torch
    TORCH_AVAILABLE = True
    print("âœ… PyTorchå¯ç”¨")
except ImportError:
    print("âŒ PyTorchæœªå®‰è£…")
    TORCH_AVAILABLE = False

# æ·»åŠ Grounded-SAM-2é¡¹ç›®è·¯å¾„
import sys
grounded_sam2_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'Grounded-SAM-2')
if os.path.exists(grounded_sam2_path):
    sys.path.insert(0, grounded_sam2_path)

# å°è¯•å¯¼å…¥ç›¸å…³æ¨¡å—
try:
    if TORCH_AVAILABLE:
        from sam2.build_sam import build_sam2
        from sam2.sam2_image_predictor import SAM2ImagePredictor
        from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection
        from pycocotools import mask as mask_utils
        MODELS_AVAILABLE = True
        print("âœ… æ¨¡å‹åº“å¯¼å…¥æˆåŠŸ")
    else:
        MODELS_AVAILABLE = False
        print("âŒ PyTorchä¸å¯ç”¨ï¼Œæ— æ³•åŠ è½½æ¨¡å‹")
except ImportError as e:
    print(f"âŒ æ¨¡å‹åº“å¯¼å…¥å¤±è´¥: {e}")
    MODELS_AVAILABLE = False

app = Flask(__name__)
CORS(app)

# é…ç½®
UPLOAD_FOLDER = 'auto_uploads'
ALLOWED_EXTENSIONS = {'png', 'jpg', 'jpeg', 'gif', 'bmp'}
MAX_FILE_SIZE = 10 * 1024 * 1024  # 10MB

# ç¡®ä¿ä¸Šä¼ ç›®å½•å­˜åœ¨
os.makedirs(UPLOAD_FOLDER, exist_ok=True)

class AutoAnnotationService:
    """è‡ªåŠ¨æ ‡æ³¨æœåŠ¡"""
    
    def __init__(self):
        self.sam2_predictor = None
        self.grounding_model = None
        self.grounding_processor = None
        self.device = "cuda" if (TORCH_AVAILABLE and torch.cuda.is_available()) else "cpu"
        
        if MODELS_AVAILABLE:
            self._initialize_models()
        else:
            print("âš ï¸ æ¨¡å‹ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿç»“æœ")
    
    def _initialize_models(self):
        """åˆå§‹åŒ–æ¨¡å‹"""
        try:
            print("ğŸ”„ æ­£åœ¨åˆå§‹åŒ–æ¨¡å‹...")
            
            # åˆå§‹åŒ–SAM2
            sam2_configs = [
                ("sam2.1_hiera_large.pt", "sam2/configs/sam2.1/sam2.1_hiera_l.yaml"),
                ("sam2_hiera_large.pt", "sam2/configs/sam2/sam2_hiera_l.yaml"),
                ("sam2.1_hiera_base_plus.pt", "sam2/configs/sam2.1/sam2.1_hiera_b+.yaml"),
            ]
            
            sam2_loaded = False
            for checkpoint_name, config_path in sam2_configs:
                checkpoint_path = os.path.join(grounded_sam2_path, "checkpoints", checkpoint_name)
                full_config_path = os.path.join(grounded_sam2_path, config_path)
                
                if os.path.exists(checkpoint_path) and os.path.exists(full_config_path):
                    print(f"ğŸ”„ åŠ è½½SAM2: {checkpoint_name}")
                    sam2_model = build_sam2(full_config_path, checkpoint_path, device=self.device)
                    self.sam2_predictor = SAM2ImagePredictor(sam2_model)
                    sam2_loaded = True
                    print(f"âœ… SAM2åŠ è½½æˆåŠŸ: {checkpoint_name}")
                    break
            
            if not sam2_loaded:
                print("âŒ æœªæ‰¾åˆ°SAM2æ¨¡å‹æ–‡ä»¶")
                self.sam2_predictor = None
            
            # åˆå§‹åŒ–Grounding DINO
            try:
                print("ğŸ”„ æ­£åœ¨åŠ è½½Grounding DINO...")
                model_id = "IDEA-Research/grounding-dino-tiny"
                self.grounding_processor = AutoProcessor.from_pretrained(model_id)
                self.grounding_model = AutoModelForZeroShotObjectDetection.from_pretrained(model_id).to(self.device)
                print("âœ… Grounding DINOåŠ è½½æˆåŠŸ")
            except Exception as e:
                print(f"âŒ Grounding DINOåŠ è½½å¤±è´¥: {e}")
                self.grounding_model = None
                self.grounding_processor = None
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            self.sam2_predictor = None
            self.grounding_model = None
            self.grounding_processor = None
    
    def detect_objects(self, image, text_prompt, box_threshold=0.35, text_threshold=0.25):
        """ä½¿ç”¨Grounding DINOæ£€æµ‹å¯¹è±¡"""
        if not self.grounding_model or not self.grounding_processor:
            return self._mock_detection(image, text_prompt)
        
        try:
            print(f"ğŸ¯ æ–‡æœ¬æç¤º: '{text_prompt}'")
            
            # å¤„ç†è¾“å…¥
            inputs = self.grounding_processor(images=image, text=text_prompt, return_tensors="pt").to(self.device)
            
            # æ¨ç†
            with torch.no_grad():
                outputs = self.grounding_model(**inputs)
            
            # åå¤„ç†
            results = self.grounding_processor.post_process_grounded_object_detection(
                outputs,
                inputs.input_ids,
                box_threshold=box_threshold,
                text_threshold=text_threshold,
                target_sizes=[image.size[::-1]]
            )
            
            if len(results) > 0 and len(results[0]["boxes"]) > 0:
                boxes = results[0]["boxes"].cpu().numpy()
                scores = results[0]["scores"].cpu().numpy()
                labels = results[0]["labels"]
                
                print(f"ğŸ“Š æ£€æµ‹åˆ° {len(boxes)} ä¸ªå¯¹è±¡")
                return boxes, scores, labels
            else:
                print("âš ï¸ æœªæ£€æµ‹åˆ°åŒ¹é…çš„å¯¹è±¡")
                return np.array([]), np.array([]), []
                
        except Exception as e:
            print(f"âŒ å¯¹è±¡æ£€æµ‹å¤±è´¥: {e}")
            return self._mock_detection(image, text_prompt)
    
    def _mock_detection(self, image, text_prompt):
        """æ¨¡æ‹Ÿæ£€æµ‹ç»“æœ"""
        print("ğŸ”„ ä½¿ç”¨æ¨¡æ‹Ÿæ£€æµ‹ç»“æœ")
        w, h = image.size
        
        # åˆ›å»ºä¸€äº›æ¨¡æ‹Ÿçš„æ£€æµ‹æ¡†
        mock_boxes = np.array([
            [w*0.1, h*0.1, w*0.4, h*0.4],  # å·¦ä¸Š
            [w*0.6, h*0.6, w*0.9, h*0.9],  # å³ä¸‹
        ])
        mock_scores = np.array([0.8, 0.7])
        mock_labels = [f"detected_{text_prompt}_1", f"detected_{text_prompt}_2"]
        
        return mock_boxes, mock_scores, mock_labels
    
    def segment_objects(self, image_array, boxes):
        """ä½¿ç”¨SAM2åˆ†å‰²å¯¹è±¡"""
        if not self.sam2_predictor:
            return self._mock_segmentation(image_array, boxes)
        
        try:
            print(f"ğŸ­ æ­£åœ¨åˆ†å‰² {len(boxes)} ä¸ªå¯¹è±¡...")
            
            # è®¾ç½®å›¾åƒ
            self.sam2_predictor.set_image(image_array)
            
            # åˆ†å‰²
            masks, scores, logits = self.sam2_predictor.predict(
                point_coords=None,
                point_labels=None,
                box=boxes,
                multimask_output=False,
            )
            
            # å¤„ç†ç»´åº¦
            if masks.ndim == 4:
                masks = masks.squeeze(1)
            
            print(f"âœ… åˆ†å‰²å®Œæˆï¼Œæ©ç å½¢çŠ¶: {masks.shape}")
            return masks, scores
            
        except Exception as e:
            print(f"âŒ åˆ†å‰²å¤±è´¥: {e}")
            return self._mock_segmentation(image_array, boxes)
    
    def _mock_segmentation(self, image_array, boxes):
        """æ¨¡æ‹Ÿåˆ†å‰²ç»“æœ"""
        print("ğŸ”„ ä½¿ç”¨æ¨¡æ‹Ÿåˆ†å‰²ç»“æœ")
        h, w = image_array.shape[:2]
        masks = []
        scores = []
        
        for box in boxes:
            # åˆ›å»ºç®€å•çš„çŸ©å½¢æ©ç 
            mask = np.zeros((h, w), dtype=bool)
            x1, y1, x2, y2 = box.astype(int)
            x1, y1 = max(0, x1), max(0, y1)
            x2, y2 = min(w, x2), min(h, y2)
            mask[y1:y2, x1:x2] = True
            masks.append(mask)
            scores.append(0.5)
        
        return np.array(masks), np.array(scores)
    
    def encode_mask(self, mask):
        """ç¼–ç æ©ç ä¸ºbase64æ ¼å¼ (ä¸simple_backend.pyå…¼å®¹)"""
        try:
            # ç¡®ä¿æ©ç æ˜¯uint8æ ¼å¼ï¼Œå¹¶è½¬æ¢ä¸º0/1äºŒå€¼
            binary_mask = (mask > 0).astype(np.uint8)
            
            # ä½¿ç”¨ä¸simple_backend.pyç›¸åŒçš„ç¼–ç æ–¹å¼
            mask_flat = binary_mask.flatten()
            mask_bytes = mask_flat.tobytes()
            encoded = base64.b64encode(mask_bytes).decode('utf-8')
            
            return {
                'size': [int(mask.shape[0]), int(mask.shape[1])],  # [height, width]
                'counts': encoded  # ä¸å‰ç«¯å…¼å®¹çš„base64æ ¼å¼
            }
        except Exception as e:
            print(f"æ©ç ç¼–ç å¤±è´¥: {e}")
            # è¿”å›ç©ºæ©ç 
            return {
                'size': [int(mask.shape[0]), int(mask.shape[1])],
                'counts': ""
            }
    
    def auto_annotate(self, image_path, text_prompt, box_threshold=0.35, text_threshold=0.25):
        """æ‰§è¡Œè‡ªåŠ¨æ ‡æ³¨"""
        try:
            # åŠ è½½å›¾åƒ
            image = Image.open(image_path).convert("RGB")
            image_array = np.array(image)
            
            print(f"ğŸ“¸ å›¾åƒå°ºå¯¸: {image.size}")
            
            # æ£€æµ‹å¯¹è±¡
            boxes, detection_scores, labels = self.detect_objects(
                image, text_prompt, box_threshold, text_threshold
            )
            
            if len(boxes) == 0:
                return {
                    'success': True,
                    'message': 'æœªæ£€æµ‹åˆ°åŒ¹é…çš„å¯¹è±¡',
                    'objects': [],
                    'image_info': {
                        'width': image.size[0],
                        'height': image.size[1]
                    }
                }
            
            # åˆ†å‰²å¯¹è±¡
            masks, segmentation_scores = self.segment_objects(image_array, boxes)
            
            # å¤„ç†ç»“æœ
            results = []
            for i in range(len(boxes)):
                box = boxes[i]
                mask = masks[i]
                label = labels[i] if i < len(labels) else f"object_{i}"
                det_score = detection_scores[i] if i < len(detection_scores) else 0.0
                seg_score = segmentation_scores[i] if i < len(segmentation_scores) else 0.0
                
                # ç¼–ç æ©ç 
                encoded_mask = self.encode_mask(mask)
                
                result = {
                    'object_id': i + 1,
                    'label': label,
                    'bbox': [float(box[0]), float(box[1]), float(box[2]), float(box[3])],
                    'detection_score': float(det_score),
                    'segmentation_score': float(seg_score),
                    'mask': encoded_mask
                }
                results.append(result)
            
            return {
                'success': True,
                'message': f'æˆåŠŸæ£€æµ‹å¹¶åˆ†å‰²äº† {len(results)} ä¸ªå¯¹è±¡',
                'objects': results,
                'image_info': {
                    'width': image.size[0],
                    'height': image.size[1]
                },
                'text_prompt': text_prompt,
                'thresholds': {
                    'box_threshold': box_threshold,
                    'text_threshold': text_threshold
                }
            }
            
        except Exception as e:
            print(f"âŒ è‡ªåŠ¨æ ‡æ³¨å¤±è´¥: {e}")
            return {
                'success': False,
                'error': str(e),
                'message': 'è‡ªåŠ¨æ ‡æ³¨è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯'
            }

# åˆå§‹åŒ–æœåŠ¡
annotation_service = AutoAnnotationService()

def allowed_file(filename):
    """æ£€æŸ¥æ–‡ä»¶æ‰©å±•å"""
    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS

@app.route('/api/health', methods=['GET'])
def health_check():
    """å¥åº·æ£€æŸ¥"""
    return jsonify({
        'status': 'healthy',
        'torch_available': TORCH_AVAILABLE,
        'models_available': MODELS_AVAILABLE,
        'sam2_ready': annotation_service.sam2_predictor is not None,
        'grounding_ready': annotation_service.grounding_model is not None,
        'device': annotation_service.device
    })

@app.route('/api/auto_annotate', methods=['POST'])
def auto_annotate():
    """è‡ªåŠ¨æ ‡æ³¨æ¥å£"""
    try:
        # æ£€æŸ¥è¯·æ±‚æ ¼å¼
        if 'image' not in request.files:
            return jsonify({'error': 'æ²¡æœ‰å›¾åƒæ–‡ä»¶'}), 400
        
        if 'text_prompt' not in request.form:
            return jsonify({'error': 'æ²¡æœ‰æ–‡æœ¬æç¤º'}), 400
        
        file = request.files['image']
        text_prompt = request.form['text_prompt'].strip()
        
        if file.filename == '':
            return jsonify({'error': 'æ²¡æœ‰é€‰æ‹©æ–‡ä»¶'}), 400
        
        if not text_prompt:
            return jsonify({'error': 'æ–‡æœ¬æç¤ºä¸èƒ½ä¸ºç©º'}), 400
        
        if not allowed_file(file.filename):
            return jsonify({'error': 'ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼'}), 400
        
        # æ£€æŸ¥æ–‡ä»¶å¤§å°
        file.seek(0, os.SEEK_END)
        file_size = file.tell()
        file.seek(0)
        
        if file_size > MAX_FILE_SIZE:
            return jsonify({'error': 'æ–‡ä»¶è¿‡å¤§'}), 400
        
        # è·å–å¯é€‰å‚æ•°
        box_threshold = float(request.form.get('box_threshold', 0.35))
        text_threshold = float(request.form.get('text_threshold', 0.25))
        
        # ä¿å­˜ä¸´æ—¶æ–‡ä»¶
        filename = str(uuid.uuid4()) + '.' + file.filename.rsplit('.', 1)[1].lower()
        filepath = os.path.join(UPLOAD_FOLDER, filename)
        file.save(filepath)
        
        print(f"ğŸ”„ å¼€å§‹è‡ªåŠ¨æ ‡æ³¨: '{text_prompt}'")
        
        # æ‰§è¡Œè‡ªåŠ¨æ ‡æ³¨
        result = annotation_service.auto_annotate(
            filepath, text_prompt, box_threshold, text_threshold
        )
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        try:
            os.remove(filepath)
        except:
            pass
        
        return jsonify(result)
        
    except Exception as e:
        print(f"âŒ APIé”™è¯¯: {e}")
        return jsonify({
            'success': False,
            'error': str(e),
            'message': 'APIè°ƒç”¨å¤±è´¥'
        }), 500

@app.route('/api/batch_annotate', methods=['POST'])
def batch_annotate():
    """æ‰¹é‡è‡ªåŠ¨æ ‡æ³¨æ¥å£"""
    try:
        if 'images' not in request.files:
            return jsonify({'error': 'æ²¡æœ‰å›¾åƒæ–‡ä»¶'}), 400
        
        if 'text_prompt' not in request.form:
            return jsonify({'error': 'æ²¡æœ‰æ–‡æœ¬æç¤º'}), 400
        
        files = request.files.getlist('images')
        text_prompt = request.form['text_prompt'].strip()
        
        if not files:
            return jsonify({'error': 'æ²¡æœ‰é€‰æ‹©æ–‡ä»¶'}), 400
        
        if not text_prompt:
            return jsonify({'error': 'æ–‡æœ¬æç¤ºä¸èƒ½ä¸ºç©º'}), 400
        
        # è·å–å¯é€‰å‚æ•°
        box_threshold = float(request.form.get('box_threshold', 0.35))
        text_threshold = float(request.form.get('text_threshold', 0.25))
        
        results = []
        
        for i, file in enumerate(files):
            if not allowed_file(file.filename):
                results.append({
                    'filename': file.filename,
                    'success': False,
                    'error': 'ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼'
                })
                continue
            
            # ä¿å­˜ä¸´æ—¶æ–‡ä»¶
            filename = f"{uuid.uuid4()}_{i}.{file.filename.rsplit('.', 1)[1].lower()}"
            filepath = os.path.join(UPLOAD_FOLDER, filename)
            file.save(filepath)
            
            # æ‰§è¡Œæ ‡æ³¨
            result = annotation_service.auto_annotate(
                filepath, text_prompt, box_threshold, text_threshold
            )
            result['filename'] = file.filename
            results.append(result)
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            try:
                os.remove(filepath)
            except:
                pass
        
        return jsonify({
            'success': True,
            'message': f'æ‰¹é‡å¤„ç†å®Œæˆï¼Œå…±å¤„ç† {len(results)} ä¸ªæ–‡ä»¶',
            'results': results
        })
        
    except Exception as e:
        print(f"âŒ æ‰¹é‡APIé”™è¯¯: {e}")
        return jsonify({
            'success': False,
            'error': str(e),
            'message': 'æ‰¹é‡APIè°ƒç”¨å¤±è´¥'
        }), 500

@app.errorhandler(413)
def too_large(e):
    return jsonify({'error': 'æ–‡ä»¶è¿‡å¤§'}), 413

if __name__ == '__main__':
    print("ğŸš€ å¯åŠ¨è‡ªåŠ¨æ ‡æ³¨æœåŠ¡...")
    print(f"ğŸ”§ PyTorchå¯ç”¨: {TORCH_AVAILABLE}")
    print(f"ğŸ¯ æ¨¡å‹å¯ç”¨: {MODELS_AVAILABLE}")
    print(f"ğŸ“ ä¸Šä¼ ç›®å½•: {UPLOAD_FOLDER}")
    print("ğŸŒ æœåŠ¡åœ°å€: http://localhost:5001")
    print("ğŸ’¡ APIç«¯ç‚¹:")
    print("   POST /api/auto_annotate - å•å›¾è‡ªåŠ¨æ ‡æ³¨")
    print("   POST /api/batch_annotate - æ‰¹é‡è‡ªåŠ¨æ ‡æ³¨")
    print("   GET /api/health - å¥åº·æ£€æŸ¥")
    print("â¹ï¸ æŒ‰ Ctrl+C åœæ­¢æœåŠ¡")
    
    app.run(host='0.0.0.0', port=5001, debug=True)